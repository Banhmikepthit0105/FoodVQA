{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pip install the necessary packages\n",
    "# !pip install nltk\n",
    "# !pip install rouge_score\n",
    "# !pip install openai\n",
    "# !pip install google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "import numpy as np\n",
    "import nltk\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download NLTK data\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path\n",
    "input = r'test_data.csv'\n",
    "output_folder = r'output/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer</th>\n",
       "      <th>predicted_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The sun rises in the east every morning</td>\n",
       "      <td>The sun rises the a east every morning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The sun rises in the east every morning</td>\n",
       "      <td>The sun the in east every morning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Python is a popular programming language</td>\n",
       "      <td>popular a is programming language</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Python is a popular programming language</td>\n",
       "      <td>Python is a popular programming language</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Artificial intelligence is transforming the world</td>\n",
       "      <td>Artificial the is transforming world</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Python is a popular programming language</td>\n",
       "      <td>language is a popular programming Python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Artificial intelligence is transforming the world</td>\n",
       "      <td>transforming intelligence Artificial the world</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The sun rises in the east every morning</td>\n",
       "      <td>The rises Python every the east in morning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The sun rises in the east every morning</td>\n",
       "      <td>every sun rises in the east The morning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The sun rises in the east every morning</td>\n",
       "      <td>The rises in every east over the morning</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              answer  \\\n",
       "0            The sun rises in the east every morning   \n",
       "1            The sun rises in the east every morning   \n",
       "2           Python is a popular programming language   \n",
       "3           Python is a popular programming language   \n",
       "4  Artificial intelligence is transforming the world   \n",
       "5           Python is a popular programming language   \n",
       "6  Artificial intelligence is transforming the world   \n",
       "7            The sun rises in the east every morning   \n",
       "8            The sun rises in the east every morning   \n",
       "9            The sun rises in the east every morning   \n",
       "\n",
       "                                 predicted_answer  \n",
       "0          The sun rises the a east every morning  \n",
       "1               The sun the in east every morning  \n",
       "2               popular a is programming language  \n",
       "3        Python is a popular programming language  \n",
       "4            Artificial the is transforming world  \n",
       "5        language is a popular programming Python  \n",
       "6  transforming intelligence Artificial the world  \n",
       "7      The rises Python every the east in morning  \n",
       "8         every sun rises in the east The morning  \n",
       "9        The rises in every east over the morning  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save to pandas dataframe\n",
    "df = pd.read_csv(input)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluates the similarity between a predicted text and a reference text based on n-gram precision.\n",
    "How it is calculated:\n",
    "- Tokenize each sentence.\n",
    "- Count matching n-gram (usually 1-4).\n",
    "- Applies a penalty if `len(prediction)` < `len(answer)`.\n",
    "\n",
    "In this code:\n",
    "- BLEU-4 is used via `sentence_bleu` function from nltk: Each n-gram from 1 to 4 contributes equally to the final score.\n",
    "- `reference`: The target (goal) answers.\n",
    "- `hypothesis`: Generated answers.\n",
    "- `smoothie`: From nltk library to handle short sentences where high n-grams return no matches.\n",
    "\n",
    "The BLEU score is computed as:\n",
    "$$ BLEU = BP * exp(\\sum^{N}_{n=1}w_n log(p_n)) $$\n",
    "- BP: The penalty where:\n",
    "    + BP = 1 if $l_{pred} \\geq l_{ref}$\n",
    "    + BP = $e^{1 - \\frac{l_{ref}}{l_{pred}}}$ if $l_{pred} < l_{ref}$\n",
    "    + $l_X$: length of X\n",
    "- $p_n$: Count matching n-gram between predictions and reference.\n",
    "    + Clip (fit) the count to the maximum number of times each n-gram appears in the reference to avoid overcounting.\n",
    "    + $p_n$ = (sum of clipped n-gram counts) / (total n-grams in prediction).\n",
    "- $w_n$: Weight of each n-gram. In BLEU-4, each n-gram has equal weight of 0.25.\n",
    "- The exp function converts it back to a score of 0-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bleu(reference, hypothesis):\n",
    "    \"\"\"Calculate BLEU score for a single pair of sentences\"\"\"\n",
    "    ref_words = str(reference).strip().split()\n",
    "    hyp_words = str(hypothesis).strip().split()\n",
    "    \n",
    "    if not ref_words or not hyp_words:\n",
    "        return 0.0\n",
    "        \n",
    "    smoothie = SmoothingFunction().method4\n",
    "    return sentence_bleu([ref_words], hyp_words, smoothing_function=smoothie)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLEU Score: 0.3230\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer</th>\n",
       "      <th>predicted_answer</th>\n",
       "      <th>bleu_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The sun rises in the east every morning</td>\n",
       "      <td>The sun rises the a east every morning</td>\n",
       "      <td>0.288540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The sun rises in the east every morning</td>\n",
       "      <td>The sun the in east every morning</td>\n",
       "      <td>0.228941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Python is a popular programming language</td>\n",
       "      <td>popular a is programming language</td>\n",
       "      <td>0.124787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Python is a popular programming language</td>\n",
       "      <td>Python is a popular programming language</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Artificial intelligence is transforming the world</td>\n",
       "      <td>Artificial the is transforming world</td>\n",
       "      <td>0.124787</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              answer  \\\n",
       "0            The sun rises in the east every morning   \n",
       "1            The sun rises in the east every morning   \n",
       "2           Python is a popular programming language   \n",
       "3           Python is a popular programming language   \n",
       "4  Artificial intelligence is transforming the world   \n",
       "\n",
       "                           predicted_answer  bleu_score  \n",
       "0    The sun rises the a east every morning    0.288540  \n",
       "1         The sun the in east every morning    0.228941  \n",
       "2         popular a is programming language    0.124787  \n",
       "3  Python is a popular programming language    1.000000  \n",
       "4      Artificial the is transforming world    0.124787  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Change the column named 'answer' and 'predicted_answer' if necessary\n",
    "if df is not None:\n",
    "    bleu_scores = [calculate_bleu(row['answer'], row['predicted_answer']) \n",
    "                  for _, row in df.iterrows()]\n",
    "    avg_bleu = np.mean(bleu_scores)\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    bleu_results = pd.DataFrame({\n",
    "        'answer': df['answer'], # Reference - Change here\n",
    "        'predicted_answer': df['predicted_answer'], # Hypothesis - Change here\n",
    "        'bleu_score': bleu_scores\n",
    "    })\n",
    "    \n",
    "    print(f\"Average BLEU Score: {avg_bleu:.4f}\")\n",
    "    display(bleu_results.head())\n",
    "    bleu_results.to_csv(output_folder + 'bleu_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rouge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluates the similarity between a predicted text and a reference text based on recall of n-grams or longest common subsequences. How it works:\n",
    "- Tokenize each sentence into words.\n",
    "- Compute overlap between predictions and references:\n",
    "    + `ROUGE-1` measures overlap of unigrams based on recall.\n",
    "    + `ROUGE-2` measures overlap of bigrams.\n",
    "    + `ROUGE-L` measures the longest common subsequence (LCS).\n",
    "- Focuses on recall (fraction of reference content captured), reports F1 scores (harmonic mean of precision and recall).\n",
    "\n",
    "In this code:\n",
    "- `rouge_scorer` is used from the `rouge_score` library with stemming to normalize words.\n",
    "- Calculates three variants:\n",
    "    + rouge1: Unigram overlap.\n",
    "    + rouge2: Bigram overlap.\n",
    "    + rougeL: LCS-based similarity.\n",
    "- `reference`: The target (goal) answers.\n",
    "- `hypothesis`: Generated answers.\n",
    "\n",
    "The ROUGE score is computed as:\n",
    "\n",
    "ROUGE-N = (Number of overlapping N-grams) / (Total N-grams in reference)\n",
    "- This is the recall score. In practice, F1 is reported:\n",
    "F1 = 2*(Precision * Recall) / (Precision + Recall).\n",
    "    + Precision = (Number of overlapping N-grams) / (Total N-grams in reference)\n",
    "    + Recall = (Number of overlapping N-grams) / (Total N-grams in reference)\n",
    "\n",
    "ROUGE-L = LCS(reference, prediction) / (Total words in reference)\n",
    "- LCS: Length of LCS\n",
    "- F1 is also computed using precision (LCS length / prediction length) and recall (LCS length / reference length).\n",
    "- Scores range from 0-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rouge(reference, hypothesis):\n",
    "    \"\"\"Calculate ROUGE scores for a single pair of sentences\"\"\"\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(str(reference), str(hypothesis))\n",
    "    return {\n",
    "        'rouge1': scores['rouge1'].fmeasure,\n",
    "        'rouge2': scores['rouge2'].fmeasure,\n",
    "        'rougeL': scores['rougeL'].fmeasure\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ROUGE-1 Score: 0.9286\n",
      "Average ROUGE-2 Score: 0.4157\n",
      "Average ROUGE-L Score: 0.7160\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer</th>\n",
       "      <th>predicted_answer</th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The sun rises in the east every morning</td>\n",
       "      <td>The sun rises the a east every morning</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The sun rises in the east every morning</td>\n",
       "      <td>The sun the in east every morning</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Python is a popular programming language</td>\n",
       "      <td>popular a is programming language</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.545455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Python is a popular programming language</td>\n",
       "      <td>Python is a popular programming language</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Artificial intelligence is transforming the world</td>\n",
       "      <td>Artificial the is transforming world</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.727273</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              answer  \\\n",
       "0            The sun rises in the east every morning   \n",
       "1            The sun rises in the east every morning   \n",
       "2           Python is a popular programming language   \n",
       "3           Python is a popular programming language   \n",
       "4  Artificial intelligence is transforming the world   \n",
       "\n",
       "                           predicted_answer    rouge1    rouge2    rougeL  \n",
       "0    The sun rises the a east every morning  0.875000  0.571429  0.875000  \n",
       "1         The sun the in east every morning  0.933333  0.461538  0.800000  \n",
       "2         popular a is programming language  0.909091  0.222222  0.545455  \n",
       "3  Python is a popular programming language  1.000000  1.000000  1.000000  \n",
       "4      Artificial the is transforming world  0.909091  0.222222  0.727273  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if df is not None:\n",
    "    rouge1_scores = []\n",
    "    rouge2_scores = []\n",
    "    rougeL_scores = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        scores = calculate_rouge(row['answer'], row['predicted_answer']) # Change here\n",
    "        rouge1_scores.append(scores['rouge1'])\n",
    "        rouge2_scores.append(scores['rouge2'])\n",
    "        rougeL_scores.append(scores['rougeL'])\n",
    "    \n",
    "    # Calculate averages\n",
    "    avg_rouge1 = np.mean(rouge1_scores)\n",
    "    avg_rouge2 = np.mean(rouge2_scores)\n",
    "    avg_rougeL = np.mean(rougeL_scores)\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    rouge_results = pd.DataFrame({\n",
    "        'answer': df['answer'], # Reference - Change here\n",
    "        'predicted_answer': df['predicted_answer'], # Hypothesis - Change here\n",
    "        'rouge1': rouge1_scores,\n",
    "        'rouge2': rouge2_scores,\n",
    "        'rougeL': rougeL_scores\n",
    "    })\n",
    "    \n",
    "    print(f\"Average ROUGE-1 Score: {avg_rouge1:.4f}\")\n",
    "    print(f\"Average ROUGE-2 Score: {avg_rouge2:.4f}\")\n",
    "    print(f\"Average ROUGE-L Score: {avg_rougeL:.4f}\")\n",
    "    display(rouge_results.head())\n",
    "    rouge_results.to_csv(output_folder + 'rouge_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Api keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_api_key = \"apikey\" # Changew here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPTScore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How GPTScore Works\n",
    "\n",
    "Refer to this <a href=\"https://arxiv.org/pdf/2302.04166\">paper</a>.\n",
    "\n",
    "### Evaluation Protocol\n",
    "- Define the **task** (e.g., summarization) and the **aspect** to evaluate (e.g., fluency) using specific instructions.\n",
    "- Example instruction: \"Generate a fluent summary for this text.\"\n",
    "\n",
    "### Input Construction\n",
    "- Combine the following components into a single prompt:\n",
    "  - **Source Text**: The context, such as the original document or dialogue history (if applicable).\n",
    "  - **Instruction**: A natural language description of the evaluation aspect.\n",
    "  - **Demonstrations**: Optional exemplar samples (e.g., reference-hypothesis pairs with scores) to guide the model via in-context learning.\n",
    "\n",
    "### Scoring\n",
    "- Use a generative model to compute the likelihood of the **hypothesis text** (the generated text) given the constructed prompt.\n",
    "- **Interpretation**: A higher likelihood indicates higher quality for the specified aspect.\n",
    "- The score reflects how naturally the hypothesis aligns with the context and instruction.\n",
    "\n",
    "### Meta-Evaluation\n",
    "- Validate the computed scores by correlating them with human judgments.\n",
    "- Common correlation measures include:\n",
    "  - **Spearman Correlation**: Assesses monotonic relationships.\n",
    "  - **Pearson Correlation**: Assesses linear relationships.\n",
    "\n",
    "### Conceptual Formula\n",
    "The GPTScore is conceptually defined as the conditional probability of the hypothesis given the context, instruction, and demonstrations:\n",
    "\n",
    "$$\n",
    "\\text{GPTScore} = P(\\text{hypothesis} \\mid \\text{context, instruction, demonstrations})\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- **context**: The source text or dialogue history providing the basis for evaluation.\n",
    "- **instruction**: A natural language description specifying the evaluation aspect (e.g., fluency, relevance).\n",
    "- **demonstrations**: Optional examples included in the prompt to enhance model performance through in-context learning.\n",
    "\n",
    "### Implementation Note\n",
    "- The paper approximates this probability using **log-likelihoods** or **normalized scores** derived from the generative model’s output.\n",
    "- Practical implementations may vary, such as using API-based ratings when direct log probabilities are unavailable (e.g., with newer OpenAI APIs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = gpt_api_key\n",
    "\n",
    "def calculate_gptscore(reference, hypothesis, aspect=\"fluency\", model=\"gpt-4o\", demonstrations=None):\n",
    "    \"\"\"\n",
    "    Calculate GPTScore for a hypothesis text given a reference and evaluation aspect.\n",
    "    \n",
    "    Args:\n",
    "        reference (str): The reference or source text.\n",
    "        hypothesis (str): The generated text to evaluate.\n",
    "        aspect (str): The evaluation aspect (e.g., \"fluency\", \"relevance\").\n",
    "        model (str): The GPT model to use (e.g., \"gpt-4o\", \"gpt-4-turbo\").\n",
    "        demonstrations (list): Optional list of (ref, hypo, score) tuples for in-context learning.\n",
    "    \n",
    "    Returns:\n",
    "        float: Normalized score between 0 and 1.\n",
    "    \"\"\"\n",
    "    # Define aspect-specific instruction\n",
    "    aspect_instructions = {\n",
    "        \"fluency\": \"Rate the fluency of the hypothesis based on the reference (0-1 scale).\",\n",
    "        \"relevance\": \"Rate how relevant the hypothesis is to the reference (0-1 scale).\",\n",
    "        \"informativeness\": \"Rate how informative the hypothesis is compared to the reference (0-1 scale).\"\n",
    "    }\n",
    "    \n",
    "    instruction = aspect_instructions.get(aspect, \"Evaluate the quality of this text.\")\n",
    "    prompt = f\"{instruction}\\n\\nReference: {reference}\\nHypothesis: {hypothesis}\\nScore:\"\n",
    "\n",
    "    # Add demonstrations if provided\n",
    "    messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant that evaluates text similarity.\"}]\n",
    "    \n",
    "    if demonstrations:\n",
    "        for demo_ref, demo_hypo, demo_score in demonstrations:\n",
    "            messages.append({\"role\": \"user\", \"content\": f\"Reference: {demo_ref}\\nHypothesis: {demo_hypo}\\nScore: {demo_score}\"})\n",
    "    \n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "    # Call the OpenAI API\n",
    "    client = openai.OpenAI(api_key=gpt_api_key)\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=model,  # Correct model\n",
    "        messages=messages,  # Correct API format\n",
    "        max_tokens=5,  # Short response expected (score)\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    # Extract score\n",
    "    score_text = response.choices[0].message.content.strip()\n",
    "    \n",
    "    try:\n",
    "        score = float(score_text)\n",
    "        normalized_score = min(max(score, 0), 1)  # Clip to [0,1]\n",
    "    except ValueError:\n",
    "        normalized_score = 0.5  # Default score if GPT fails to return a valid number\n",
    "    \n",
    "    return normalized_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average GPTScore: 0.5000\n",
      "                                              answer  \\\n",
      "0            The sun rises in the east every morning   \n",
      "1            The sun rises in the east every morning   \n",
      "2           Python is a popular programming language   \n",
      "3           Python is a popular programming language   \n",
      "4  Artificial intelligence is transforming the world   \n",
      "5           Python is a popular programming language   \n",
      "6  Artificial intelligence is transforming the world   \n",
      "7            The sun rises in the east every morning   \n",
      "8            The sun rises in the east every morning   \n",
      "9            The sun rises in the east every morning   \n",
      "\n",
      "                                 predicted_answer  gpt_score  \n",
      "0          The sun rises the a east every morning        0.5  \n",
      "1               The sun the in east every morning        0.5  \n",
      "2               popular a is programming language        0.5  \n",
      "3        Python is a popular programming language        0.5  \n",
      "4            Artificial the is transforming world        0.5  \n",
      "5        language is a popular programming Python        0.5  \n",
      "6  transforming intelligence Artificial the world        0.5  \n",
      "7      The rises Python every the east in morning        0.5  \n",
      "8         every sun rises in the east The morning        0.5  \n",
      "9        The rises in every east over the morning        0.5  \n"
     ]
    }
   ],
   "source": [
    "demonstrations = [\n",
    "    (\"The cat sits on the mat.\", \"The cat rests on the rug.\", 0.9),\n",
    "    (\"The dog barks loudly.\", \"The loud dog barks.\", 0.8)\n",
    "]\n",
    "\n",
    "gpt_scores = []\n",
    "for _, row in df.iterrows():\n",
    "    score = calculate_gptscore(\n",
    "        reference=row['answer'], # Change here \n",
    "        hypothesis=row['predicted_answer'], # Change here\n",
    "        model=\"gpt-4o-mini\",\n",
    "        demonstrations=demonstrations\n",
    "    )\n",
    "    gpt_scores.append(score)\n",
    "\n",
    "# Calculate average GPT score\n",
    "avg_gpt = np.mean(gpt_scores)\n",
    "print(f\"Average GPTScore: {avg_gpt:.4f}\")\n",
    "\n",
    "# Add scores to DataFrame and save\n",
    "df['gpt_score'] = gpt_scores\n",
    "print(df)\n",
    "df.to_csv(output_folder + 'gptscore_results.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
