{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pip install the necessary packages\n",
    "# !pip install nltk\n",
    "# !pip install rouge_score\n",
    "# !pip install openai\n",
    "# !pip install google-generativeai\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "import numpy as np\n",
    "import nltk\n",
    "import openai\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download NLTK data\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path\n",
    "input = r'test_predict_beit3.csv'\n",
    "output_folder = r'output/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image</th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "      <th>Predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ff557f4ea977d3a</td>\n",
       "      <td>What color is the top of the enchiladas</td>\n",
       "      <td>golden-brown</td>\n",
       "      <td>golden-brown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ff557f4ea977d3a</td>\n",
       "      <td>What type of dish is positioned on the left si...</td>\n",
       "      <td>enchiladas</td>\n",
       "      <td>salad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ff557f4ea977d3a</td>\n",
       "      <td>What color is the filling of the enchiladas</td>\n",
       "      <td>white</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ff557f4ea977d3a</td>\n",
       "      <td>Where are the enchiladas located on the plate</td>\n",
       "      <td>on the left side</td>\n",
       "      <td>on the left side</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ff557f4ea977d3a</td>\n",
       "      <td>What vegetable is scattered on top of the lettuce</td>\n",
       "      <td>tomatoes</td>\n",
       "      <td>tomatoes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3694</th>\n",
       "      <td>e95d2afc654400d</td>\n",
       "      <td>Where are the turkey, spaghetti, and zoodles s...</td>\n",
       "      <td>throughout the entire pan</td>\n",
       "      <td>throughout</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3695</th>\n",
       "      <td>73c6313f570a1cd</td>\n",
       "      <td>What color are the grilled zucchini sticks</td>\n",
       "      <td>yellow</td>\n",
       "      <td>green</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3696</th>\n",
       "      <td>73c6313f570a1cd</td>\n",
       "      <td>What vegetable is shown on the platter</td>\n",
       "      <td>zucchini</td>\n",
       "      <td>zucchini</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3697</th>\n",
       "      <td>73c6313f570a1cd</td>\n",
       "      <td>What color is the pepper sprinkled on the zucc...</td>\n",
       "      <td>black</td>\n",
       "      <td>green</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3698</th>\n",
       "      <td>73c6313f570a1cd</td>\n",
       "      <td>Where are the zucchini piled on the plate</td>\n",
       "      <td>in the center</td>\n",
       "      <td>on top of</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3699 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Image                                           Question  \\\n",
       "0     ff557f4ea977d3a            What color is the top of the enchiladas   \n",
       "1     ff557f4ea977d3a  What type of dish is positioned on the left si...   \n",
       "2     ff557f4ea977d3a        What color is the filling of the enchiladas   \n",
       "3     ff557f4ea977d3a      Where are the enchiladas located on the plate   \n",
       "4     ff557f4ea977d3a  What vegetable is scattered on top of the lettuce   \n",
       "...               ...                                                ...   \n",
       "3694  e95d2afc654400d  Where are the turkey, spaghetti, and zoodles s...   \n",
       "3695  73c6313f570a1cd         What color are the grilled zucchini sticks   \n",
       "3696  73c6313f570a1cd             What vegetable is shown on the platter   \n",
       "3697  73c6313f570a1cd  What color is the pepper sprinkled on the zucc...   \n",
       "3698  73c6313f570a1cd          Where are the zucchini piled on the plate   \n",
       "\n",
       "                         Answer           Predict  \n",
       "0                  golden-brown      golden-brown  \n",
       "1                    enchiladas             salad  \n",
       "2                         white             white  \n",
       "3              on the left side  on the left side  \n",
       "4                      tomatoes          tomatoes  \n",
       "...                         ...               ...  \n",
       "3694  throughout the entire pan        throughout  \n",
       "3695                     yellow             green  \n",
       "3696                   zucchini          zucchini  \n",
       "3697                      black             green  \n",
       "3698              in the center         on top of  \n",
       "\n",
       "[3699 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save to pandas dataframe\n",
    "df = pd.read_csv(input)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluates the similarity between a predicted text and a reference text based on n-gram precision.\n",
    "How it is calculated:\n",
    "- Tokenize each sentence.\n",
    "- Count matching n-gram (usually 1-4).\n",
    "- Applies a penalty if `len(prediction)` < `len(answer)`.\n",
    "\n",
    "In this code:\n",
    "- BLEU-4 is used via `sentence_bleu` function from nltk: Each n-gram from 1 to 4 contributes equally to the final score.\n",
    "- `reference`: The target (goal) answers.\n",
    "- `hypothesis`: Generated answers.\n",
    "- `smoothie`: From nltk library to handle short sentences where high n-grams return no matches.\n",
    "\n",
    "The BLEU score is computed as:\n",
    "$$ BLEU = BP * exp(\\sum^{N}_{n=1}w_n log(p_n)) $$\n",
    "- BP: The penalty where:\n",
    "    + BP = 1 if $l_{pred} \\geq l_{ref}$\n",
    "    + BP = $e^{1 - \\frac{l_{ref}}{l_{pred}}}$ if $l_{pred} < l_{ref}$\n",
    "    + $l_X$: length of X\n",
    "- $p_n$: Count matching n-gram between predictions and reference.\n",
    "    + Clip (fit) the count to the maximum number of times each n-gram appears in the reference to avoid overcounting.\n",
    "    + $p_n$ = (sum of clipped n-gram counts) / (total n-grams in prediction).\n",
    "- $w_n$: Weight of each n-gram. In BLEU-4, each n-gram has equal weight of 0.25.\n",
    "- The exp function converts it back to a score of 0-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bleu(reference, hypothesis):\n",
    "    \"\"\"Calculate BLEU score for a single pair of sentences\"\"\"\n",
    "    ref_words = str(reference).strip().split()\n",
    "    hyp_words = str(hypothesis).strip().split()\n",
    "    \n",
    "    if not ref_words or not hyp_words:\n",
    "        return 0.0\n",
    "        \n",
    "    smoothie = SmoothingFunction().method4\n",
    "    return sentence_bleu([ref_words], hyp_words, smoothing_function=smoothie)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLEU Score: 0.4797\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer</th>\n",
       "      <th>predicted_answer</th>\n",
       "      <th>bleu_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>golden-brown</td>\n",
       "      <td>golden-brown</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>enchiladas</td>\n",
       "      <td>salad</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>white</td>\n",
       "      <td>white</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>on the left side</td>\n",
       "      <td>on the left side</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tomatoes</td>\n",
       "      <td>tomatoes</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             answer  predicted_answer  bleu_score\n",
       "0      golden-brown      golden-brown         1.0\n",
       "1        enchiladas             salad         0.0\n",
       "2             white             white         1.0\n",
       "3  on the left side  on the left side         1.0\n",
       "4          tomatoes          tomatoes         1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Change the column named 'answer' and 'predicted_answer' if necessary\n",
    "if df is not None:\n",
    "    bleu_scores = [calculate_bleu(row['Answer'], row['Predict']) \n",
    "                  for _, row in df.iterrows()]\n",
    "    avg_bleu = np.mean(bleu_scores)\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    bleu_results = pd.DataFrame({\n",
    "        'answer': df['Answer'], # Reference - Change here\n",
    "        'predicted_answer': df['Predict'], # Hypothesis - Change here\n",
    "        'bleu_score': bleu_scores\n",
    "    })\n",
    "    \n",
    "    print(f\"Average BLEU Score: {avg_bleu:.4f}\")\n",
    "    display(bleu_results.head())\n",
    "    bleu_results.to_csv(output_folder + 'bleu_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rouge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluates the similarity between a predicted text and a reference text based on recall of n-grams or longest common subsequences. How it works:\n",
    "- Tokenize each sentence into words.\n",
    "- Compute overlap between predictions and references:\n",
    "    + `ROUGE-1` measures overlap of unigrams based on recall.\n",
    "    + `ROUGE-2` measures overlap of bigrams.\n",
    "    + `ROUGE-L` measures the longest common subsequence (LCS).\n",
    "- Focuses on recall (fraction of reference content captured), reports F1 scores (harmonic mean of precision and recall).\n",
    "\n",
    "In this code:\n",
    "- `rouge_scorer` is used from the `rouge_score` library with stemming to normalize words.\n",
    "- Calculates three variants:\n",
    "    + rouge1: Unigram overlap.\n",
    "    + rouge2: Bigram overlap.\n",
    "    + rougeL: LCS-based similarity.\n",
    "- `reference`: The target (goal) answers.\n",
    "- `hypothesis`: Generated answers.\n",
    "\n",
    "The ROUGE score is computed as:\n",
    "\n",
    "ROUGE-N = (Number of overlapping N-grams) / (Total N-grams in reference)\n",
    "- This is the recall score. In practice, F1 is reported:\n",
    "F1 = 2*(Precision * Recall) / (Precision + Recall).\n",
    "    + Precision = (Number of overlapping N-grams) / (Total N-grams in reference)\n",
    "    + Recall = (Number of overlapping N-grams) / (Total N-grams in reference)\n",
    "\n",
    "ROUGE-L = LCS(reference, prediction) / (Total words in reference)\n",
    "- LCS: Length of LCS\n",
    "- F1 is also computed using precision (LCS length / prediction length) and recall (LCS length / reference length).\n",
    "- Scores range from 0-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rouge(reference, hypothesis):\n",
    "    \"\"\"Calculate ROUGE scores for a single pair of sentences\"\"\"\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(str(reference), str(hypothesis))\n",
    "    return {\n",
    "        'rouge1': scores['rouge1'].fmeasure,\n",
    "        'rouge2': scores['rouge2'].fmeasure,\n",
    "        'rougeL': scores['rougeL'].fmeasure\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ROUGE-1 Score: 0.5781\n",
      "Average ROUGE-2 Score: 0.0853\n",
      "Average ROUGE-L Score: 0.5780\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer</th>\n",
       "      <th>predicted_answer</th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>golden-brown</td>\n",
       "      <td>golden-brown</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>enchiladas</td>\n",
       "      <td>salad</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>white</td>\n",
       "      <td>white</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>on the left side</td>\n",
       "      <td>on the left side</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tomatoes</td>\n",
       "      <td>tomatoes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             answer  predicted_answer  rouge1  rouge2  rougeL\n",
       "0      golden-brown      golden-brown     1.0     1.0     1.0\n",
       "1        enchiladas             salad     0.0     0.0     0.0\n",
       "2             white             white     1.0     0.0     1.0\n",
       "3  on the left side  on the left side     1.0     1.0     1.0\n",
       "4          tomatoes          tomatoes     1.0     0.0     1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if df is not None:\n",
    "    rouge1_scores = []\n",
    "    rouge2_scores = []\n",
    "    rougeL_scores = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        scores = calculate_rouge(row['Answer'], row['Predict']) # Change here\n",
    "        rouge1_scores.append(scores['rouge1'])\n",
    "        rouge2_scores.append(scores['rouge2'])\n",
    "        rougeL_scores.append(scores['rougeL'])\n",
    "    \n",
    "    # Calculate averages\n",
    "    avg_rouge1 = np.mean(rouge1_scores)\n",
    "    avg_rouge2 = np.mean(rouge2_scores)\n",
    "    avg_rougeL = np.mean(rougeL_scores)\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    rouge_results = pd.DataFrame({\n",
    "        'answer': df['Answer'], # Reference - Change here\n",
    "        'predicted_answer': df['Predict'], # Hypothesis - Change here\n",
    "        'rouge1': rouge1_scores,\n",
    "        'rouge2': rouge2_scores,\n",
    "        'rougeL': rougeL_scores\n",
    "    })\n",
    "    \n",
    "    print(f\"Average ROUGE-1 Score: {avg_rouge1:.4f}\")\n",
    "    print(f\"Average ROUGE-2 Score: {avg_rouge2:.4f}\")\n",
    "    print(f\"Average ROUGE-L Score: {avg_rougeL:.4f}\")\n",
    "    display(rouge_results.head())\n",
    "    rouge_results.to_csv(output_folder + 'beit3_rouge_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Api keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_api_key = \"apikey\" # Changew here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPTScore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How GPTScore Works\n",
    "\n",
    "Refer to this <a href=\"https://arxiv.org/pdf/2302.04166\">paper</a>.\n",
    "\n",
    "### Evaluation Protocol\n",
    "- Define the **task** (e.g., summarization) and the **aspect** to evaluate (e.g., fluency) using specific instructions.\n",
    "- Example instruction: \"Generate a fluent summary for this text.\"\n",
    "\n",
    "### Input Construction\n",
    "- Combine the following components into a single prompt:\n",
    "  - **Source Text**: The context, such as the original document or dialogue history (if applicable).\n",
    "  - **Instruction**: A natural language description of the evaluation aspect.\n",
    "  - **Demonstrations**: Optional exemplar samples (e.g., reference-hypothesis pairs with scores) to guide the model via in-context learning.\n",
    "\n",
    "### Scoring\n",
    "- Use a generative model to compute the likelihood of the **hypothesis text** (the generated text) given the constructed prompt.\n",
    "- **Interpretation**: A higher likelihood indicates higher quality for the specified aspect.\n",
    "- The score reflects how naturally the hypothesis aligns with the context and instruction.\n",
    "\n",
    "### Meta-Evaluation\n",
    "- Validate the computed scores by correlating them with human judgments.\n",
    "- Common correlation measures include:\n",
    "  - **Spearman Correlation**: Assesses monotonic relationships.\n",
    "  - **Pearson Correlation**: Assesses linear relationships.\n",
    "\n",
    "### Conceptual Formula\n",
    "The GPTScore is conceptually defined as the conditional probability of the hypothesis given the context, instruction, and demonstrations:\n",
    "\n",
    "$$\n",
    "\\text{GPTScore} = P(\\text{hypothesis} \\mid \\text{context, instruction, demonstrations})\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- **context**: The source text or dialogue history providing the basis for evaluation.\n",
    "- **instruction**: A natural language description specifying the evaluation aspect (e.g., fluency, relevance).\n",
    "- **demonstrations**: Optional examples included in the prompt to enhance model performance through in-context learning.\n",
    "\n",
    "### Implementation Note\n",
    "- The paper approximates this probability using **log-likelihoods** or **normalized scores** derived from the generative modelâ€™s output.\n",
    "- Practical implementations may vary, such as using API-based ratings when direct log probabilities are unavailable (e.g., with newer OpenAI APIs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = gpt_api_key\n",
    "\n",
    "def calculate_gptscore(reference, hypothesis, aspect=\"relevance\", model=\"gpt-4o\", demonstrations=None):\n",
    "    \"\"\"\n",
    "    Calculate GPTScore for a hypothesis text given a reference and evaluation aspect.\n",
    "    \n",
    "    Args:\n",
    "        reference (str): The reference or source text.\n",
    "        hypothesis (str): The generated text to evaluate.\n",
    "        aspect (str): The evaluation aspect (e.g., \"fluency\", \"relevance\").\n",
    "        model (str): The GPT model to use (e.g., \"gpt-4o\", \"gpt-4-turbo\").\n",
    "        demonstrations (list): Optional list of (ref, hypo, score) tuples for in-context learning.\n",
    "    \n",
    "    Returns:\n",
    "        float: Normalized score between 0 and 1.\n",
    "    \"\"\"\n",
    "    # Define aspect-specific instruction\n",
    "    aspect_instructions = {\n",
    "        \"fluency\": \"Rate the fluency of the hypothesis based on the reference (0-1 scale).\",\n",
    "        \"relevance\": \"Rate how relevant the hypothesis is to the reference (0-1 scale).\",\n",
    "        \"informativeness\": \"Rate how informative the hypothesis is compared to the reference (0-1 scale).\"\n",
    "    }\n",
    "    \n",
    "    instruction = aspect_instructions.get(aspect, \"Evaluate the quality of this text.\")\n",
    "    prompt = f\"{instruction}\\n\\nReference: {reference}\\nHypothesis: {hypothesis}\\nScore:\"\n",
    "\n",
    "    # Add demonstrations if provided\n",
    "    messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant that evaluates text similarity. Respond with only the score and nothing else.\"}]\n",
    "    \n",
    "    if demonstrations:\n",
    "        for demo_ref, demo_hypo, demo_score in demonstrations:\n",
    "            messages.append({\"role\": \"user\", \"content\": f\"Reference: {demo_ref}\\nHypothesis: {demo_hypo}\\nScore: {demo_score}\"})\n",
    "    \n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "    # Call the OpenAI API\n",
    "    client = openai.OpenAI(api_key=gpt_api_key)\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=model,  # Correct model\n",
    "        messages=messages,  # Correct API format\n",
    "        max_tokens=5,  # Short response expected (score)\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    # Extract score\n",
    "    score_text = response.choices[0].message.content.strip()\n",
    "    \n",
    "    try:\n",
    "        score = float(score_text)\n",
    "        normalized_score = min(max(score, 0), 1)  # Clip to [0,1]\n",
    "    except ValueError:\n",
    "        normalized_score = 0.5  # Default score if GPT fails to return a valid number\n",
    "    \n",
    "    return normalized_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average GPTScore: 0.5000\n",
      "                                              answer  \\\n",
      "0            The sun rises in the east every morning   \n",
      "1            The sun rises in the east every morning   \n",
      "2           Python is a popular programming language   \n",
      "3           Python is a popular programming language   \n",
      "4  Artificial intelligence is transforming the world   \n",
      "5           Python is a popular programming language   \n",
      "6  Artificial intelligence is transforming the world   \n",
      "7            The sun rises in the east every morning   \n",
      "8            The sun rises in the east every morning   \n",
      "9            The sun rises in the east every morning   \n",
      "\n",
      "                                 predicted_answer  gpt_score  \n",
      "0          The sun rises the a east every morning        0.5  \n",
      "1               The sun the in east every morning        0.5  \n",
      "2               popular a is programming language        0.5  \n",
      "3        Python is a popular programming language        0.5  \n",
      "4            Artificial the is transforming world        0.5  \n",
      "5        language is a popular programming Python        0.5  \n",
      "6  transforming intelligence Artificial the world        0.5  \n",
      "7      The rises Python every the east in morning        0.5  \n",
      "8         every sun rises in the east The morning        0.5  \n",
      "9        The rises in every east over the morning        0.5  \n"
     ]
    }
   ],
   "source": [
    "demonstrations = [\n",
    "    (\"The cat sits on the mat.\", \"The cat rests on the rug.\", 0.9),\n",
    "    (\"The dog barks loudly.\", \"The loud dog barks.\", 0.8)\n",
    "]\n",
    "\n",
    "gpt_scores = []\n",
    "for _, row in df.iterrows():\n",
    "    score = calculate_gptscore(\n",
    "        reference=row['answer'], # Change here \n",
    "        hypothesis=row['predicted_answer'], # Change here\n",
    "        model=\"gpt-4o-mini\",\n",
    "        demonstrations=demonstrations\n",
    "    )\n",
    "    print(_, score)\n",
    "    gpt_scores.append(score)\n",
    "\n",
    "# Calculate average GPT score\n",
    "avg_gpt = np.mean(gpt_scores)\n",
    "print(f\"Average GPTScore: {avg_gpt:.4f}\")\n",
    "\n",
    "# Add scores to DataFrame and save\n",
    "df['gpt_score'] = gpt_scores\n",
    "print(df)\n",
    "df.to_csv(output_folder + 'gptscore_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics: EM, Precision, Recall, and F1\n",
    "\n",
    "These metrics assess the quality of predicted text against reference text at a token level, complementing n-gram-based (BLEU, ROUGE) and semantic (GPTScore) evaluations.\n",
    "\n",
    "### Exact Match (EM)\n",
    "- **Definition**: Measures if the predicted text exactly matches the reference text (case-insensitive, stripped of extra whitespace).\n",
    "- **Formula**: \\( EM = 1 \\) if \\( \\text{predicted} == \\text{reference} \\), else \\( 0 \\).\n",
    "- **Use**: Strict measure of correctness, useful for tasks requiring identical outputs.\n",
    "\n",
    "### Precision\n",
    "- **Definition**: Fraction of tokens in the predicted text that appear in the reference text.\n",
    "- **Formula**: \n",
    "\\[\n",
    "\\text{Precision} = \\frac{|\\text{predicted tokens} \\cap \\text{reference tokens}|}{|\\text{predicted tokens}|}\n",
    "\\]\n",
    "- **Use**: Highlights over-generation (extra tokens in prediction).\n",
    "\n",
    "### Recall\n",
    "- **Definition**: Fraction of tokens in the reference text captured by the predicted text.\n",
    "- **Formula**: \n",
    "\\[\n",
    "\\text{Recall} = \\frac{|\\text{predicted tokens} \\cap \\text{reference tokens}|}{|\\text{reference tokens}|}\n",
    "\\]\n",
    "- **Use**: Highlights under-generation (missing tokens from reference).\n",
    "\n",
    "### F1 Score\n",
    "- **Definition**: Harmonic mean of precision and recall, balancing both metrics.\n",
    "- **Formula**: \n",
    "\\[\n",
    "F1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "\\]\n",
    "- **Use**: Provides a single score for overall token-level performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_em(reference, hypothesis):\n",
    "    \"\"\"Calculate Exact Match score.\"\"\"\n",
    "    ref_clean = \" \".join(reference.strip().lower().split())\n",
    "    hyp_clean = \" \".join(hypothesis.strip().lower().split())\n",
    "    return 1.0 if ref_clean == hyp_clean else 0.0\n",
    "\n",
    "def calculate_precision_recall_f1(reference, hypothesis):\n",
    "    \"\"\"Calculate Precision, Recall, and F1 scores based on token overlap.\"\"\"\n",
    "    ref_tokens = set(word_tokenize(reference.lower()))\n",
    "    hyp_tokens = set(word_tokenize(hypothesis.lower()))\n",
    "    \n",
    "    if not hyp_tokens:  # Avoid division by zero\n",
    "        precision = 0.0 if ref_tokens else 1.0\n",
    "    else:\n",
    "        precision = len(ref_tokens & hyp_tokens) / len(hyp_tokens)\n",
    "    \n",
    "    if not ref_tokens:  # Avoid division by zero\n",
    "        recall = 1.0 if not hyp_tokens else 0.0\n",
    "    else:\n",
    "        recall = len(ref_tokens & hyp_tokens) / len(ref_tokens)\n",
    "    \n",
    "    if precision + recall == 0:  # Avoid division by zero\n",
    "        f1 = 0.0\n",
    "    else:\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Exact Match (EM): 0.0000\n",
      "Average Precision: 0.7222\n",
      "Average Recall: 0.7667\n",
      "Average F1 Score: 0.7424\n",
      "Results saved to 'evaluation_results_em_prf1.csv'\n"
     ]
    }
   ],
   "source": [
    "# Load or create sample data\n",
    "def create_test_csv(filename=\"test_data.csv\"):\n",
    "    data = {\n",
    "        \"answer\": [\n",
    "            \"The quick brown fox jumps over the lazy dog\",\n",
    "            \"I enjoy coding in Python\",\n",
    "            \"Machine learning is fascinating\"\n",
    "        ],\n",
    "        \"predicted_answer\": [\n",
    "            \"The quick brown fox leaps over the idle dog\",\n",
    "            \"I like coding in Python daily\",\n",
    "            \"Machine learning is interesting\"\n",
    "        ]\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(filename, index=False)\n",
    "    return df\n",
    "\n",
    "# Load data\n",
    "try:\n",
    "    df = pd.read_csv(input) \n",
    "except FileNotFoundError:\n",
    "    df = create_test_csv(\"test_data.csv\")\n",
    "\n",
    "# Compute metrics\n",
    "em_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    ref = row['answer']\n",
    "    hyp = row['predicted_answer']\n",
    "    \n",
    "    # Exact Match\n",
    "    em = calculate_em(ref, hyp)\n",
    "    em_scores.append(em)\n",
    "    \n",
    "    # Precision, Recall, F1\n",
    "    precision, recall, f1 = calculate_precision_recall_f1(ref, hyp)\n",
    "    precision_scores.append(precision)\n",
    "    recall_scores.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "# Calculate averages\n",
    "avg_em = np.mean(em_scores)\n",
    "avg_precision = np.mean(precision_scores)\n",
    "avg_recall = np.mean(recall_scores)\n",
    "avg_f1 = np.mean(f1_scores)\n",
    "\n",
    "# Print results\n",
    "print(f\"Average Exact Match (EM): {avg_em:.4f}\")\n",
    "print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "print(f\"Average Recall: {avg_recall:.4f}\")\n",
    "print(f\"Average F1 Score: {avg_f1:.4f}\")\n",
    "\n",
    "# Save results to DataFrame and CSV\n",
    "results_df = df.copy()\n",
    "results_df['EM'] = em_scores\n",
    "results_df['Precision'] = precision_scores\n",
    "results_df['Recall'] = recall_scores\n",
    "results_df['F1'] = f1_scores\n",
    "results_df.to_csv(output_folder + \"evaluation_results_em_prf1.csv\", index=False) # Save to CSV\n",
    "print(\"Results saved to 'evaluation_results_em_prf1.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
