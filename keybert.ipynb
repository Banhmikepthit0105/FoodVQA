{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import error: No module named 'triton'\n"
     ]
    }
   ],
   "source": [
    "from keybert import KeyBERT\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "kw_model = KeyBERT('all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords(description, stop_words=stopwords.words('english')):\n",
    "    return id, kw_model.extract_keywords(description, keyphrase_ngram_range=(1, 1), stop_words=stop_words)\n",
    "def process_batch(batch_des, batch_id_image, batch_id, timeout=10):\n",
    "    keywords = []\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        future_to_description = {executor.submit(extract_keywords, batch_des[i]): i for i in range(len(batch_des))}  \n",
    "        for future in as_completed(future_to_description, timeout=timeout):\n",
    "            try:\n",
    "                keyword_tuples = future.result()[1]  # Extracts the full keyword list (with scores)\n",
    "                keyword_list = [word for word, _ in keyword_tuples]  # Extract only the words\n",
    "\n",
    "                keywords.append({\n",
    "                    \"id\": batch_id[future_to_description[future]],\n",
    "                    \"id_image\": batch_id_image[future_to_description[future]],\n",
    "                    \"keywords\": keyword_list\n",
    "                })\n",
    "                # print(f\"id_image {batch_id_image[future_to_description[future]]} id {batch_id[future_to_description[future]]} processed , reuslt: {keyword_list} \")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing description: {e}\")\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('merge.csv')\n",
    "descriptions = df['Image description'].tolist()\n",
    "ids_image = df['Image'].tolist()\n",
    "id = np.arange(0, len(descriptions))\n",
    "batch_size = 10\n",
    "all_keywords = []\n",
    "for i in range(0, len(descriptions), batch_size):\n",
    "    batch_des = df['Image description'][i:i+batch_size].tolist()\n",
    "    batch_id_image = df['Image'][i:i+batch_size].tolist()\n",
    "    id = np.arange(i, i+batch_size)\n",
    "    all_keywords.extend(process_batch(batch_des,batch_id_image, id))\n",
    "    print(f\"Processed {i+batch_size} descriptions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort all keywords dict by id\n",
    "all_keywords = sorted(all_keywords, key=lambda x: x['id'])\n",
    "pd.to_csv('keywords.csv', all_keywords)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
